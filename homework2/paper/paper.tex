\documentclass[runningheads]{paper}

\usepackage{hyperref}   % hyperlinks
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{float}
\usepackage{array}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

% ------------------------------------------------------------------------------

\begin{document}
%
\title{Exploring the Utility of Machine Learning Across Varied Data Formats}
%
\author{Cărămidă Iustina-Andreea - 332CA}
%
\institute{Faculty of Automatic Control and Computer Science \\
University Politehnica of Bucharest \\
\email{iustina.caramida@stud.acs.upb.ro}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
    This study investigates the applicability of machine learning techniques on 
    diverse datasets. We explore the effectiveness of two algorithms, Logistic 
    Regression and Multi-Layered Perceptron (MLP), on predicting both health 
    outcomes and financial well-being. Specifically, we utilize a stroke 
    prediction dataset to assess the model's ability to identify individuals at 
    risk of stroke. Additionally, we employ a salary prediction dataset to 
    evaluate the model's capacity to classify individuals earning above a 
    specific income threshold (e.g., \$50,000 per year). Through comparative 
    analysis, this research aims to elucidate the strengths and limitations of 
    each algorithm when applied to these contrasting data types, offering 
    insights into their suitability for various prediction tasks. Furthermore, 
    we present a framework for data analysis, outlining essential steps for data
    cleaning, exploration, and preparation, which can be applied to enhance the 
    effectiveness of machine learning models across diverse datasets.

\keywords{\textit{Machine Learning \and Heterogeneous Data \and Comparative Analysis \and
Prediction Modeling \and Data Analysis Techniques \and Stroke Prediction \and
Salary Prediction \and Logistic Regression \and Multi-Layered Perceptron \and 
DataPreprocessing} }

\end{abstract}
% ------------------------------------------------------------------------------
\section{Introduction}

\subsection{Motivation: The Power and Nuance of Machine Learning Data}
Machine learning (ML) has become a cornerstone of progress in numerous 
disciplines. Its ability to extract valuable insights from vast and complex 
datasets has fueled breakthroughs in healthcare, finance, and social sciences. 
However, the effectiveness of ML models is not a one-size-fits-all proposition. 
Different data types possess unique characteristics, and understanding these 
nuances is essential for selecting the most appropriate ML algorithms. Data can 
be structured (organized in tables) or unstructured (text, images), numerical 
or categorical, and may exhibit linear or non-linear relationships between 
features. Choosing the right algorithm depends heavily on these factors. This 
research delves into this crucial aspect of ML application by exploring the 
performance of two distinct algorithms on contrasting datasets.

\subsection{Research Focus: Delving into Stroke Prediction and Salary Prediction}
This study focuses on the application of ML techniques to two contrasting 
datasets: stroke prediction and salary prediction. Stroke, a leading cause of 
disability and death globally, poses a significant public health burden. Stroke 
prediction models aim to identify individuals at high risk of experiencing a 
stroke, allowing for preventive measures and early intervention. These models 
typically analyze factors such as age, blood pressure, cholesterol levels, and 
smoking history.

Conversely, salary prediction models attempt to classify individuals based on 
income thresholds. This information can offer valuable insights into economic 
trends, such as income inequality, and inform policy decisions. Salary 
prediction models might analyze factors like education level, work experience, 
and industry sector. By investigating these two distinct datasets, this research 
aims to gain a broader understanding of how ML algorithms perform on different 
data types with varying underlying structures and complexities.

\subsection{Methodology: Unveiling the Algorithms - Logistic Regression and Multi-Layered Perceptron}
This section delves into the core methodologies employed in this study: 
Logistic Regression and Multi-Layered Perceptron (MLP). Both algorithms fall 
under the umbrella of supervised learning, where a model learns from labeled 
data to make predictions on unseen examples. Here, we unveil the underlying 
principles and functionalities of each technique.

Logistic regression serves as a foundational algorithm for classification tasks.
It establishes a mathematical model that maps input features (e.g., age, blood 
pressure) to a probability of a specific outcome (e.g., stroke occurrence). The 
model essentially learns a decision boundary, separating observations with high 
and low probabilities of the target outcome. This approach makes logistic 
regression well-suited for analyzing datasets like the stroke prediction one,
where the goal is to categorize individuals based on their risk level.

On the other hand, Multi-Layered Perceptron (MLP) represents a more complex 
architecture - a type of artificial neural network. It consists of interconnected 
layers of artificial neurons, mimicking the structure of the human brain. Each 
layer transforms the received data using activation functions, ultimately leading 
to an output prediction. MLP's strength lies in its ability to learn complex, 
non-linear relationships within data. This makes it a powerful tool for tackling 
intricate prediction problems, potentially outperforming logistic regression 
when the underlying relationships are not easily captured by a linear model. 
The salary prediction dataset, where various factors might influence income 
levels, could be a prime candidate for exploration with MLP.

\subsection{Research Objectives: Evaluating Algorithms, Unveiling Strengths and Weaknesses}
By comparatively analyzing the performance of Logistic Regression and MLP on 
stroke and salary prediction tasks, this research seeks to achieve several key 
objectives:

\subsubsection{Evaluate the Suitability of Algorithms for Diverse Data Types:}
This involves assessing the effectiveness of each algorithm in capturing the 
underlying relationships within the stroke prediction and salary prediction 
datasets. We will determine which algorithm performs better on each dataset, 
offering insights into their suitability for different data types.

\subsubsection{Gain Insights into Algorithmic Strengths and Weaknesses:}
By analyzing the comparative performance, we aim to highlight the scenarios where
each algorithm excels and identify areas where one might outperform the other. 
This will provide valuable guidance for researchers and practitioners in 
selecting the most appropriate algorithm for their specific prediction tasks. 

\subsubsection{Demonstrate Best Practices for Data Analysis in ML Applications:}
Effective data analysis is crucial for building robust ML models. This research 
will showcase essential steps for data cleaning, exploration, and preparation, 
emphasizing their importance in enhancing model performance across diverse 
datasets. These steps may include handling missing values, identifying outliers, 
and feature engineering (creating new features from existing data) to improve 
the model's ability to learn from the data.

\subsection{Expected Contribution: Advancing the Application of ML on Heterogeneous Data}
Through this exploration, the research aims to contribute valuable knowledge to 
the field of machine learning, particularly the application of ML on 
heterogeneous datasets.  The findings can guide researchers and practitioners 
in selecting appropriate algorithms for their specific prediction tasks and 
data types.  Furthermore, by demonstrating best practices for data analysis, 
this research can contribute to the development of more robust and reliable ML 
models across diverse application domains. This can lead to advancements in 
areas like healthcare (improved stroke prediction for preventive measures) and 
economics (better understanding of factors influencing income inequality).  
Ultimately, the research aims to contribute to the responsible and effective use 
of ML for tackling complex problems across various fields.
% ------------------------------------------------------------------------------
\section{Exploratory Data Analysis}

\subsection{Datasets attributes description}
The initial and crucial step in developing any machine learning algorithm 
involves a thorough understanding of the data it will be trained on. This 
understanding is achieved through a comprehensive analysis of the datasets' 
characteristics. In this vein, the following sub sections will delve into the 
specific attributes of the two datasets employed in this study: stroke 
prediction and salary prediction.

A detailed description of each salary prediction attribute is provided in Table
\ref{tab:salary_attributes_description} and of each stroke  prediction attribute
in Table \ref{tab:stroke_attributes_description}.

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}p{3cm}||>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{6cm}| }
        \hline
        \multicolumn{3}{|c|}{List of all attributes in the Salary Prediction dataset} \\
        \hline
        Attribute name & Type & Details \\
        \hline
        fnl & numeric & Socio-economic characteristic of the population from which the individual comes \\
        \hline
        hpw & numeric & Number of work hours per week \\
        \hline
        relation & categorical & The type of relationship in which the individual is involved \\
        \hline
        gain & numeric & Capital gain \\
        \hline
        country & categorical & Country of origin \\
        \hline
        job & categorical & The individual's job \\
        \hline
        edu\_int & numeric & Number of years of study \\
        \hline
        years & numeric & Age of the individual \\
        \hline
        loss & numeric & Loss of capital \\
        \hline
        work\_type & categorical & The job's type \\
        \hline
        partner & categorical & The type of partner the individual has \\
        \hline
        edu & categorical & The individual's type of education \\
        \hline
        gender & categorical & Individual's gender \\
        \hline
        race & categorical & Individual's race \\
        \hline
        prod & numeric & Capital production \\
        \hline
        gtype & categorical & Type of employment contract \\
        \hline
        money & categorical & Whether the individual earns more than \$50,000 per year \\
        \hline
        \caption{Salary Prediction Attributes}
        \label{tab:salary_attributes_description}
   \end{longtable}
\end{center}

\pagebreak

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{3.5cm}||>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{3.5cm}|>{\centering\arraybackslash}m{2.5cm}| }
        \hline
        \multicolumn{4}{|c|}{List of all attributes in the Stroke Prediction dataset} \\
        \hline
        Attribute name & Type & Details & Possible values \\
        \hline
        mean\_blood\_sugar\_ level & numeric & The average value of blood glucose throughout the duration observation of the subject & \\
        \hline
        cardiovascular\_issues & categorical & Whether or not the subject has a medical history cardiovascular & 0, 1 \\
        \hline
        job\_category & categorical & The field in which the person works & child, entrepreneurial, N\_work\_history, private\_sector, public\_sector \\
        \hline
        body\_mass\_indicator & numeric & Body mass index, which indicates if the person is underweight, within limits normal, overweight or obese & \\
        \hline
        sex & categorical & The gender of the person & F, M\\
        \hline
        tobacco\_usage & categorical & Current or past smoker indicator &  ex-smoker, smoker, non-smoker\\
        \hline
        high\_blood\_pressure & categorical & Binary attribute indicating whether a person suffer from high blood pressure or not & 0, 1 \\
        \hline
        married & categorical & Binary attribute indicating whether the person a ever been married & Y, N \\
        \hline
        living\_area & categorical & The type of area where he lived most of his life & City, Countryside \\
        \hline
        years\_old & numeric & The person's age in years & \\
        \hline
        chaotic\_sleep & categorical & Binary attribute for a sleep program irregular & 0, 1 \\
        \hline
        analysis\_results & numeric & The results of medical analyzes of the person, which may include various measurements and indicators relevant to her health & \\
        \hline
        biological\_age\_index & numeric & An index that estimates the biological age of a person based on different factors such as lifestyle, health status, measured in an unknown unit & \\
        \hline
        cerebrovascular\_ accident & categorical & Binary indicator indicating whether the person a had a stroke or not & 0, 1 \\
        \hline
        \caption{Stroke Prediction Attributes}
        \label{tab:stroke_attributes_description}
   \end{longtable}
\end{center}

\subsection{Exploration of Attribute Types and Value Ranges}
Prior to applying a machine learning model to a dataset, a crucial step involves
in identifying the types of attributes (features) present and their corresponding
values ranges. This anaysis is essential for selecting appropriate algorithms 
and ensuring optimal model performance. In the following pharagraphs we will
describe three primary attribute types.
\begin{itemize}
    \item \textit{Continuous Numeric Attributes:}
    These attributes possess numerical values that can theoretically take on any 
    value within a specific range. Examples might include: age, weight, temperature etc.
    \item \textit{Discrete Nominal Attributes:}
    These attributes represent categorical data with distinct, non-ordered values. 
    Examples include days of the week (Monday, Tuesday, etc.) or types of diseases 
    (cancer, diabetes, etc.).
    \item \textit{Ordinal Attributes:}
    These attributes represent categorical data with values that exhibit an inherent 
    order. However, the difference between consecutive values may not be interpretable 
    in terms of a consistent unit.  Examples include customer satisfaction ratings 
    (1-star, 2-star, etc.) or movie ratings (G, PG, PG-13, etc.). In ordinal 
    attributes, the numerical value itself might not be as important as the relative 
    order it represents.
\end{itemize}

Using the \textit{analysis\_attributes.py} script, we can identify the 
Continuous Numeric Attributes and Discrete Nominal Attributes in the  
datasets. The script will output statistics that can be showed in Tables 
\ref{tab:continuous_numeric_attributes_salary} and 
\ref{tab:continuous_numeric_attributes_stroke} for numeric attributes and
Table \ref{tab:discrete_nominal_attributes_salary} and 
\ref{tab:discrete_nominal_attributes_stroke} for discrete attributes.

Moreover, the total number of items in the full dataset is 9999 for the Salary
Prediction dataset and 5110 for the Stroke Prediction dataset.


\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{0.95cm}||>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm}|>{\centering\arraybackslash}m{1.75cm} |}
        \hline
        \multicolumn{8}{|c|}{List of all Continuous Numeric Attributes in the Salary Prediction dataset} \\
        \hline
         & fnl & hpw & gain & edu\_int & years & loss & prod \\
        \hline\hline
        count & 9.999000e +03 & 9199.00000 & 9999.00000 & 9999.00000 & 9999.00000 & 9999.00000 & 9999.00000 \\
        \hline
        mean &  1.903529e +05 & 40.416241 & 979.853385 & 14.262026 & 38.646865 & 84.111411 & 2014.9275 93 \\
        \hline
        std & 1.060709e +05 & 12.517356 & 7003.7953 82 & 24.770835 & 13.745101 & 3394.0354 84 & 14007.6044 96 \\
        \hline
        min & 1.921400e +04 & 1.000000 & 0.000000 & 1.000000 & 17.000000 & 0.000000 & -28.000000 \\
        \hline
        25\% & 1.182825e +05 & 40.000000 & 0.000000 & 9.000000 & 28.000000 & 0.000000 & 42.000000 \\
        \hline
        50\% & 1.784720e +05 & 40.000000 & 0.000000 & 10.000000 & 37.000000 & 0.000000 & 57.000000 \\
        \hline
        75\% & 2.373110e +05 & 45.000000 & 0.000000 & 13.000000 & 48.000000 & 0.000000 & 77.000000 \\
        \hline
        max & 1.455435e +06 & 99.00000 & 99999.0000 & 206.000000 & 90.000000 & 3770.00000 & 200125.000 \\
        \hline
        \caption{Continuous Numeric Attributes in Salary Prediction Dataset}
        \label{tab:continuous_numeric_attributes_salary} \\
   \end{longtable}
\end{center}

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{0.95cm}||>{\centering\arraybackslash}m{2.45cm}|>{\centering\arraybackslash}m{2.45cm}|>{\centering\arraybackslash}m{2.45cm}|>{\centering\arraybackslash}m{2.45cm}|>{\centering\arraybackslash}m{2.45cm}| }
        \hline
        \multicolumn{6}{|c|}{List of all Continuous Numeric Attributes in the Stroke Prediction dataset} \\
        \hline
        & mean\_blood\_ sugar\_level & body\_mass\_ indicator & years\_old & analysis\_results & biological\_age\_ index \\
        \hline\hline
        count & 5110.000000 & 4909.000000 & 5110.000000 & 4599.000000 & 5110.000000 \\
        \hline
        mean & 106.147677 & 28.893237 & 46.568665 & 323.523446 & 134.784256 \\
        \hline
        std & 45.283560 & 7.854067 & 26.593912 & 101.577442 & 50.399352 \\
        \hline
        min & 55.120000 & 10.300000 & 0.080000 & 104.829714 & -15.109456 \\
        \hline
        25\% & 77.245000 & 23.500000 & 26.000000 & 254.646209 & 96.710581 \\
        \hline
        50\% & 91.885000 & 28.100000 & 47.000000 & 301.031628 & 136.374631 \\
        \hline
        75\% & 114.090000 & 33.100000 & 63.750000 & 362.822769 & 172.507322 \\
        \hline
        max & 271.740000 & 97.600000 & 134.000000 & 756.807975 & 266.986321 \\
        \hline
        \caption{Continuous Numeric Attributes in Stroke Prediction Dataset}
        \label{tab:continuous_numeric_attributes_stroke} \\
    \end{longtable}
\end{center}

An initial inspection of the data reveals that there are missing attributes in 
both the Salary Prediction and Stroke Prediction datasets. In the Salary Prediction 
datase the \textit{'hpw'} attribute is missing, while int the Stroke Prediction dataset
two attributes are missing: \textit{'body\_mass\_indicator'} and 
\textit{'analysis\_results'}.

To better understand the distribution of the continuous numeric attributes 
within the datasets, boxplots have been generated for each attribute. These 
visualizations are located in the \textit{'plots'} folder at the root of the 
project directory. The name of each boxplot starts with \textit{'box\_plot\_'}.

Boxplots are a standardized method for visually representing the distribution of
data.  They provide insights into several key characteristics of the data, 
including the median, quartiles, and outliers.

In the Figure \ref{fig:boxplot_example_salary} we can see a boxplot for the 
\textit{years} attribute in the Salary Prediction dataset. The box in the middle
of the plot contains the middle 50\% of the data, and the line in the middle
represents the median. The whiskers extend to the minimum and maximum values
within 1.5 times the interquartile range (the difference between the first and
third quartiles). Points outside this range are considered outliers.

Also, in Figure \ref{fig:boxplot_example_stroke} we can see a boxplot for the
\textit{body\_mass\_indicator} attribute in the Stroke Prediction dataset.
As described above, the boxplot provides a visual representation of the data's
distribution, highlighting key statistical measures such as the median, quartiles,
and potential outliers. These information is also presented in Tables
\ref{tab:continuous_numeric_attributes_salary} and
\ref{tab:continuous_numeric_attributes_stroke}. One of the main insights that
can be derived from the boxplot is the presence of outliers, which are data points
that lie significantly outside the range of the rest of the data. Outliers can
have a significant impact on the performance of machine learning models, and
identifying and handling them appropriately is an essential step in the data
preprocessing process.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/box_plot_years_SalaryPrediction_full.png}
    \caption{Boxplot for the \textit{years} attribute in the Salary Prediction dataset}
    \label{fig:boxplot_example_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/box_plot_body_mass_indicator_AVC_full.png}
    \caption{Boxplot for the \textit{body\_mass\_indicator} attribute in the Stroke Prediction dataset}
    \label{fig:boxplot_example_stroke}
\end{figure}

From the Discret Nominal Attributes tables (Tables \ref{tab:discrete_nominal_attributes_salary} and \ref{tab:discrete_nominal_attributes_stroke})
we can see that each dataset contains
only one attribute with missing values. In the Salary Prediction dataset 
the \textit{gender} attribute is missing, while the Stroke Prediction dataset the
\textit{married} attribute is missing. Also, the number of unique values for each attribute
descibes the diversity of the data. For example, the \textit{country} attribute
in the Salary Prediction dataset has 41 unique values, indicating that the data
contains information from 41 different countries.

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{5cm}||>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|}
        \hline
        \multicolumn{3}{|c|}{List of all Discrete Nominal Attributes in the Salary Prediction dataset} \\
        \hline
        & Non-missing count & Unique values count \\
        \hline\hline
        relation & 9999 & 6 \\
        \hline
        country & 9999 & 41 \\
        \hline
        job & 9999 & 14 \\
        \hline
        work\_type & 9999 & 9 \\
        \hline
        partner & 9999 & 7 \\
        \hline
        edu & 9999 & 16 \\
        \hline
        gender & 9199 & 2 \\
        \hline
        race & 9999 & 5 \\
        \hline
        gtype & 9999 & 2 \\
        \hline
        money & 9999 & 2 \\
        \hline
        \caption{Discrete Nominal Attributes in Salary Prediction Dataset}
        \label{tab:discrete_nominal_attributes_salary} \\
    \end{longtable}
\end{center}

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{5cm}||>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{3cm}|}
        \hline
        \multicolumn{3}{|c|}{List of all Discrete Nominal Attributes in the Stroke Prediction dataset} \\
        \hline
        & Non-missing count & Unique values count \\
        \hline\hline
        cardiovascular\_issues & 5110 & 2 \\
        \hline
        job\_category & 5110 & 5 \\
        \hline
        sex & 5110 & 2 \\
        \hline
        tobacco\_usage & 5110 & 4 \\
        \hline
        high\_blood\_pressure & 5110 & 2 \\
        \hline
        married & 4599 &2 \\
        \hline
        living\_area & 5110 & 2 \\
        \hline
        chaotic\_sleep & 5110 & 2 \\
        \hline
        cerebrovascular\_accident & 5110 & 2 \\
        \hline

        \caption{Discrete Nominal Attributes in Stroke Prediction Dataset}
        \label{tab:discrete_nominal_attributes_stroke} \\
    \end{longtable}
\end{center}

\pagebreak

In the historigrams for the discrete nominal attributes, we can see the
distribution of the unique values for each attribute. These visualizations can
provide insights into the frequency of each category within the dataset, which
can be useful for understanding the data's composition and identifying potential
imbalances or biases. The histograms for the discrete nominal attributes are
located in the \textit{'plots'} folder at the root of the project directory.
The name of each histogram starts with \textit{'histogram\_'}.

In Figure \ref{fig:histogram_example_salary} we can see a histogram of the
\textit{work\_type} attribute in the Salary Prediction dataset. The dominance of
the 'Priv' category indicates a severe class imbalance. For classification tasks,
the model might predict Priv most of the time since it's the majority class, 
leading to a high overall accuracy but poor precision, recall, and F1 scores 
for minority classes.

Also, in Figure \ref{fig:histogram_example_stroke} we can see a histogram of the
\textit{tobacco\_usage} attribute in the Stroke Prediction dataset. The histogram 
shows that the majority of individuals are non-smokers, with a significant portion 
having undefined tobacco usage status. This imbalance and the presence of missing 
data need to be addressed appropriately.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/histogram_work_type_SalaryPrediction_full.png}
    \caption{Histogram for the \textit{work\_type} attribute in the Salary Prediction dataset}
    \label{fig:histogram_example_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/histogram_tobacco_usage_AVC_full.png}
    \caption{Histogram for the \textit{tobacco\_usage} attribute in the Stroke Prediction dataset}
    \label{fig:histogram_example_stroke}
\end{figure}

\subsection{Investigation of Class Distribution}
In machine learning, it is common practice to split a dataset into two distinct
subsets: a training set and a test set. This division is crucial for ensuring 
 robustness and generalizability of the models developed using the data.

\begin{itemize}
    \item \textbf{Training Set:} The primary purpose of the training set is to 
    train the machine 
    learning model. The model learns from patterns and relationships within the data 
    to develop a predictive capability.
    \item \textbf{Test Set:} The test set, unseen by the model during training, 
    serves to evaluate the model's generalizability. By applying the trained 
    model to the test set, we can assess its performance on new, unseen data. 
    This helps prevent overfitting, where the model performs well on the 
    training data but fails to generalize to real-world scenarios.
\end{itemize}

Looking at how data is distributed is key. Imbalanced data, where some classes 
have far more examples than others, throws off classification tasks:
high accuracy can hide poor performance on rare classes; models struggle to 
learn patterns from underrepresented classes; inaccurate predictions, especially 
for the minority class.

By checking the distribution, we can address imbalance:
\begin{itemize}
    \item Balance the data: Oversample rare examples or undersample common ones.
    \item Cost-sensitive learning: Penalize the model more for mistakes on rare classes.
    \item Better metrics: Use precision, recall, and F1-score to get a clearer picture.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/distribution_SalaryPrediction.png}
    \caption{Distribution of the \textit{money} class in the Salary Prediction dataset}
    \label{fig:distribution_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/distribution_AVC.png}
    \caption{Distribution of the \textit{cerebrovascular\_accident} class in the Stroke Prediction dataset}
    \label{fig:distribution_stroke}
\end{figure}

In Figures \ref{fig:distribution_salary} and \ref{fig:distribution_stroke} we can
see the distribution of each class in the datasets. The class distributions provide
insights into the balance of the data and can help guide the selection of appropriate
strategies for handling imbalanced classes. For example, in the Stroke Prediction
dataset, the \textit{cerebrovascular\_accident} class is highly imbalanced, with
a significantly higher number of negative instances compared to positive instances.
This imbalance can impact the model's ability to learn patterns from the minority
class and may require resampling techniques or cost-sensitive learning to address.
On the other hand, the Salary Prediction dataset exhibits a more balanced distribution
of the \textit{money} class, which may require less intervention to handle class
imbalance.

\subsection{Analysis of Feature Correlations}
Feature correlation analysis is a critical step in understanding the relationships
between different attributes in a dataset. By examining how attributes are related
to each other, we can identify patterns, dependencies, and redundancies that can
inform feature selection, model building, and interpretation.

Correlation analysis typically involves calculating correlation coefficients
between pairs of attributes. The correlation coefficient quantifies the strength
and direction of the linear relationship between two variables. A correlation
coefficient close to 1 indicates a strong positive relationship, while a value
close to -1 indicates a strong negative relationship. A correlation coefficient
near 0 suggests no linear relationship between the variables.

In the \textit{'correlation\_analysis.py'} script, we calculate the correlation
coefficients between all pairs of continuous numeric attributes in the datasets,
generating a correlation matrix for each dataset. Moreover, we calculate the 
Cramér's V coefficient for all pairs of discrete nominal attributes in the datasets,
generating a Cramér's V matrix for each dataset to measure the association between
categorical variables. In Figures \ref{fig:correlation_matrix_example_salary} and
\ref{fig:correlation_matrix_example_stroke} we can see the correlation matrix for
the Salary Prediction and Stroke Prediction datasets, respectively, for the continuous
numeric attributes. In Figures \ref{fig:cramer_v_matrix_example_salary} and
\ref{fig:cramer_v_matrix_example_stroke} we can see the Cramér's V matrix for
the discrete nominal attributes.

The correlation matrix and Cramér's V matrix provide valuable insights into the
relationships between attributes in the datasets. By examining these matrices, we
can see that Figure \ref{fig:correlation_matrix_example_salary} the \textit{prod}
attribute is highly correlated with the \textit{gain} attribute, while the
\textit{years} attribute is negatively correlated with the \textit{fnl} attribute.

In Figure \ref{fig:correlation_matrix_example_stroke} we can see that the
\textit{mean\_blood\_sugar\_level} attribute is highly correlated with the
\textit{analysis\_results} attribute, while the \textit{body\_mass\_indicator} 
attribute is negatively correlated with the \textit{analysis\_results} attribute.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/correlation_matrix_SalaryPrediction_full.png}
    \caption{Correlation matrix for the Salary Prediction dataset}
    \label{fig:correlation_matrix_example_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/correlation_matrix_AVC_full.png}
    \caption{Correlation matrix for the Stroke Prediction dataset}
    \label{fig:correlation_matrix_example_stroke}
\end{figure}

The Cramér's V matrix in Figures \ref{fig:cramer_v_matrix_example_salary} and
\ref{fig:cramer_v_matrix_example_stroke} provides insights into the association
between discrete nominal attributes. For example, in the Salary Prediction dataset,
the \textit{gtype} attribute is strongly associated with the \textit{gender} attribute,
while in the Stroke Prediction dataset, the \textit{cardiovascular\_issues} attribute is strongly
associated with the \textit{chaotic\_sleep} attribute.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/cramer_v_matrix_SalaryPrediction_full.png}
    \caption{Cramér's V matrix for the Salary Prediction dataset}
    \label{fig:cramer_v_matrix_example_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../plots/cramer_v_matrix_AVC_full.png}
    \caption{Cramér's V matrix for the Stroke Prediction dataset}
    \label{fig:cramer_v_matrix_example_stroke}
\end{figure}

% ------------------------------------------------------------------------------
\section{Data Preprocessing}
As highlighted in the previous section, high-quality data is the cornerstone of 
effective machine learning models. However, real-world datasets often exhibit 
various imperfections that can impede model performance.  Our exploration of the 
datasets revealed the presence of several such issues, including:

\begin{itemize}
    \item Missing values for specific attributes.
    \item Extreme values (outliers) within certain attributes.
    \item Redundant attributes with high correlation.
    \item Inconsistent value ranges for numeric attributes.
\end{itemize}

These imperfections necessitate data preprocessing, a crucial step aimed at 
transforming the raw data into a clean and consistent format. This section 
delves into the specific data preprocessing techniques employed in this study. 
By addressing these issues, we aim to optimize the data for subsequent machine 
learning algorithms, ultimately enhancing their effectiveness in extracting 
valuable insights. 

As a note, all the scripts for data preprocessing are located in the \textit{'preprocessing'}
folder at the root of the project directory.

\subsection{Handling Missing Values}
Missing data, a common issue in real-world datasets, necessitates the application
of imputation procedures to address these missing values.  Imputation techniques 
can be categorized as either univariate or multivariate:

\begin{itemize}
    \item \textit{Univariate Imputation:} This approach focuses solely on the attribute with missing 
    values. Common univariate techniques include replacing missing values with the 
    mean, median, or most frequent value within the attribute. These methods are 
    simple to implement but may not effectively capture the underlying relationships 
    between attributes.
    \item \textit{Multivariate Imputation:} This more sophisticated approach leverages the values 
    of other attributes within a sample to estimate the missing value. Techniques 
    like regression analysis are often employed to establish relationships between 
    the missing attribute and the remaining attributes. Based on these relationships, 
    a predicted value can be imputed for the missing data point.  Multivariate 
    imputation offers a more nuanced approach but requires careful consideration 
    of the relationships between attributes and potential biases in the imputation 
    process.
\end{itemize}

In the \textit{'impute\_values.py'} script, in the \textit{'missing\_values'} function,
we used the \textit{IterativeImputer}
class from the \textit{sklearn.impute} module to apply multivariate imputation to
address missing values in the datasets for continuous numeric attributes. The script
uses the most frequent value strategy for categorical attributes. The imputed datasets
are saved in the same folder as the original datasets, with the prefix
\textit{'preprocessed\_missing\_'}.

\subsection{Outlier Detection and Treatment}
Outliers, data points that deviate significantly from the rest of the dataset,
can adversely affect the performance of machine learning models. Outliers can
skew statistical measures, distort relationships between attributes, and lead to
poor generalization of the model. Detecting and treating outliers is essential
for ensuring the robustness and reliability of the model.

We purpose to impute the outliers using the \textit{IsolationForest} algorithm
from the \textit{sklearn.ensemble} module. The script \textit{'outlier\_detection.py'}
detects outliers in the continuous numeric attributes of the datasets and replaces
them with the imputed values. The preprocessed datasets with imputed outliers are
saved in the same folder as the original datasets, with the prefix
\textit{'preprocessed\_outliers\_'}.

\subsection{Analysis of Attribute Correlations}
As previously discussed, attribute correlations can provide valuable insights
into the relationships between different attributes in the dataset. By identifying
highly correlated attributes, we can eliminate redundant information and reduce
the dimensionality of the data, leading to more efficient model training and
improved interpretability.

We choose to remove highly correlated attributes found in the section of
Exploratory Data Analysis. These attributes are:

\begin{itemize}
    \item \textit{prod}: it is correlated with \textit{gain} in the Salary Prediction dataset.
    \item \textit{analysis\_results}: it is correlated with \textit{body\_mass\_indicator} in the Stroke Prediction dataset.
    \item \textit{gtype}: it is correlated with \textit{gender} in the Salary Prediction dataset.
    \item \textit{chaotic\_sleep}: it is correlated with \textit{cardiovascular\_issues} in the Stroke Prediction dataset.
\end{itemize}

The script \textit{'remove\_correlated\_attributes.py'}
removes those attributes from the train dataset and saves the preprocessed dataset
in the same folder as the original datasets, with the prefix \textit{'preprocessed\_correlated\_'}.

\subsection{Normalization and Standardization}
The numerical attributes in the dataset can vary significantly in their value scales. For example, 
some attributes may have values in the thousands, while others have values in the single digits. 
This disparity in scales can significantly affect algorithms like Logistic Regression.

In algorithms like Logistic Regression, which rely on a linear combination of attribute values,
attributes with larger numerical values can disproportionately influence the model. This dominance 
can lead to biased results and reduce the model's effectiveness.

To mitigate this issue, it is essential to standardize the values of the numeric attributes. 
Standardization adjusts the scales of the attributes, ensuring that each one contributes equally 
to the model's predictions. This process improves the performance and accuracy of the model by 
creating a more balanced and fair representation of the data.

% ------------------------------------------------------------------------------
\section{Algorithms Designs}
Algorithm design is a critical aspect of computer science and machine learning, 
focusing on creating efficient and effective methods to solve complex problems. 
The process involves the careful selection of algorithms based on the specific 
characteristics of the data and the desired outcomes. This document explores the 
application of two prominent machine learning algorithms, Logistic Regression and 
Multi-Layered Perceptron (MLP), on diverse datasets. The goal is to compare their 
performance and suitability for different types of prediction tasks, particularly 
in the contexts of stroke prediction and salary prediction.

\subsection{Logistic Regression}
Logistic regression is a fundamental statistical method employed for classification 
tasks in machine learning. It establishes a mathematical model that maps a set
of input features (independent variables) to a probability of a specific outcome 
(dependent variable). The core functionality lies in estimating the odds of a 
particular class membership (e.g., presence of stroke) based on the input 
features. The resulting model essentially learns a decision boundary, 
separating observations with high and low probabilities of belonging to the 
target class. This characteristic makes logistic regression particularly 
well-suited for analyzing datasets where the outcome variable is binary 
(e.g., stroke occurrence vs. no stroke occurrence).

In the \textit{logistic\_regression} folder at the root of the project directory,
we implemented the Logistic Regression algorithm in two different ways:

\begin{itemize}
    \item \textit{Logistic Regression with Scikit-Learn:} We used the Scikit-Learn
    library to implement Logistic Regression on the preprocessed datasets.
    \item \textit{Logistic Regression from Scratch:} We implemented Logistic Regression
    from scratch using the Negative Log-Likelihood method and the Gradient Descent
    optimization algorithm.
\end{itemize}


Before starting the implementation of the Logistic Regression algorithm, we need to
encode the categorical attributes in the datasets. Categorical attributes are non-numeric
attributes that represent discrete categories or groups. These attributes need to be
encoded into a numerical format before they can be used in machine learning algorithms.

For enconding the categorical attributes except the target attribute, I used the
\textit{OneHotEncoder} class from the \textit{sklearn.preprocessing} module. This
class encodes categorical attributes as one-hot vectors, creating a binary representation
of each category. This encoding is essential for feeding categorical attributes into
machine learning models, as most algorithms require numerical input data. For the
target attribute, I used the \textit{LabelEncoder} class from the \textit{sklearn.preprocessing}
module to encode the target attribute as integer values.

In the Logistic Regression with Scikit-Learn implementation, we used the \textit{LogisticRegression}
class from the \textit{sklearn.linear\_model} module to train the model on the preprocessed
datasets, without setting any hyperparameters, using the default values.
For the Logistic Regression from Scratch implementation, we implemented the Negative Log-Likelihood
loss function and the Gradient Descent optimization algorithm. We trained the model on the
preprocessed datasets, setting the learning rate to 0.01 and the number of epochs to 10000. For the
regularization, we used the Ridge Regression technique.

The results of both implementations for each dataset are saved in the \textit{LogisticRegression}
folder at the specific dataset's root.

\subsection{Multi-Layered Perceptron (MLP)}
A Multilayer Perceptron (MLP) is a class of feedforward artificial neural network 
that consists of at least three layers of nodes: an input layer, one or more 
hidden layers, and an output layer. Each node, except for the input nodes, is a 
neuron that uses a nonlinear activation function. MLPs are capable of modeling 
complex relationships in data, making them suitable for tasks such as 
classification, regression, and pattern recognition. The network learns by 
adjusting the weights through a process called backpropagation, which minimizes 
the error between the predicted outputs and the actual targets. This 
adaptability and learning capability make MLPs powerful tools in machine 
learning and artificial intelligence applications.

In the \textit{mlp} folder at the root of the project directory, we implemented the
Multi-Layered Perceptron algorithm in two different ways:

\begin{itemize}
    \item \textit{MLP with Scikit-Learn:} We used the Scikit-Learn library to implement
    the MLP algorithm on the preprocessed datasets.
    \item \textit{MLP from Scratch:} We implemented the MLP algorithm from scratch using
    the Negative Log-Likelihood method, the Gradient Descent optimization algorithm, and
    the Sigmoid activation function.
\end{itemize}

Before starting the implementation of the MLP algorithm, we need to standardize the
numeric attributes in the datasets as in the Logistic Regression algorithm.

For the MLP with Scikit-Learn implementation, we used the \textit{MLPClassifier} class
from the \textit{sklearn.neural\_network} module to train the model on the preprocessed
datasets, without setting any hyperparameters, using the default values.

For the MLP from Scratch implementation, we implemented the Negative Log-Likelihood loss
function, the Gradient Descent optimization algorithm, and the Sigmoid activation function.
We trained the model on the preprocessed datasets, setting the learning rate to 0.01, the
number of epochs to 10000, and the number of hidden units to 100. For the regularization,
we used the Ridge Regression technique.

The results of both implementations for each dataset are saved in the \textit{MLP}
folder at the specific dataset's root.


% ------------------------------------------------------------------------------

\section{Evaluation}
The evaluation of machine learning models is a critical step in assessing their
performance and effectiveness. By comparing the model's predictions to the actual
ground truth, we can determine the model's accuracy, precision, recall, and other
metrics that quantify its performance. This section delves into the evaluation
of the Logistic Regression and Multi-Layered Perceptron (MLP) models on the
Salary Prediction and Stroke Prediction datasets.

\subsection{Hyperparameter Tuning}
Hyperparameters are parameters that are set before the learning process begins.
They control the learning process and the behavior of the model. Hyperparameter
tuning is the process of selecting the optimal hyperparameters for a machine
learning model to achieve the best performance. This process involves searching
through different hyperparameter configurations and evaluating the model's
performance on a validation set to find the optimal settings.

In the context of the Logistic Regression (manual implementation), the 
hyperparameters that were used are:

\begin{itemize}
    \item \textit{learning\_rate:} The rate at which the model updates the weights - 0.01
    \item \textit{num\_iterations} The number of iterations the model trains for - 10000
    \item \textit{regularization} The regularization parameter to prevent overfitting - 0.1
\end{itemize}

In the context of the Logistic Regression (Scikit-Learn implementation), the majority of
hyperparameters that were used are the default values provided by the Scikit-Learn
library. The only hyperparameters that were set are:


\begin{itemize}
    \item \textit{solver:} The optimization algorithm used in the model - 
    'saga' for SalaryPrediction and 'sag' for AVC
    \item \textit{max\_iter:} The maximum number of iterations for the optimization algorithm - 200 for SalaryPrediction and 500 for AVC
    \item \textit{C:} The regularization parameter to prevent overfitting - 0.4342470001 for SalaryPrediction and  2.56050926 for AVC
\end{itemize}

In the context of the Multi-Layered Perceptron (manual implementation), the
hyperparameters that were used are:

\begin{itemize}
    \item \textit{hidden\_sizes:} The sizes of the hidden layers are defined as a list - [256, 128, 64]
    \item \textit{num\_epochs:} The number of epochs the model trains for - 100
    \item \textit{learning\_rate:} The rate at which the model updates the weights - 0.01
    \item \textit{Loss Function	:} The loss function used to optimize the model - Negative Log-Likelihood
\end{itemize}

In the context of the Multi-Layered Perceptron (Scikit-Learn implementation), the 
hyperparameters that were used are the default values provided by the Scikit-Learn
library.

\subsection{Confusion Matrix}
A confusion matrix is a table that summarizes the performance of a classification
model on a set of test data for which the true values are known. It provides
insights into the model's predictions, including true positive, true negative,
false positive, and false negative instances. These metrics are essential for
evaluating the model's performance and identifying potential areas for improvement.

In the context of the Logistic Regression and Multi-Layered Perceptron models, we
generated confusion matrices to analyze the model's predictions on the test data.
The confusion matrices provide a detailed breakdown of the model's performance,
highlighting the number of correct and incorrect predictions for each class.

In Figures \ref{fig:confusion_matrix_logistic_manual_salary} and 
\ref{fig:confusion_matrix_logistic_scikit_salary} we can see the confusion matrices
for the Logistic Regression model on the Salary Prediction dataset, implemented
manually and with Scikit-Learn, respectively. In Figures 
\ref{fig:confusion_matrix_mlp_manual_salary} and \ref{fig:confusion_matrix_mlp_scikit_salary}
we can see the confusion matrices for the Multi-Layered Perceptron model on the
Salary Prediction dataset, implemented manually and with Scikit-Learn, respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_SalaryPrediction/LogisticRegression/confusion_matrix_manual.png}
    \caption{Confusion Matrix for the Logistic Regression model on the Salary Prediction dataset (Manual Implementation)}
    \label{fig:confusion_matrix_logistic_manual_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_SalaryPrediction/LogisticRegression/confusion_matrix_scikit-learn.png}
    \caption{Confusion Matrix for the Logistic Regression model on the Salary Prediction dataset (Scikit-Learn Implementation)}
    \label{fig:confusion_matrix_logistic_scikit_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_SalaryPrediction/MLP/confusion_matrix_manual.png}
    \caption{Confusion Matrix for the Multi-Layered Perceptron model on the Salary Prediction dataset (Manual Implementation)}
    \label{fig:confusion_matrix_mlp_manual_salary}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_SalaryPrediction/MLP/confusion_matrix_scikit-learn.png}
    \caption{Confusion Matrix for the Multi-Layered Perceptron model on the Salary Prediction dataset (Scikit-Learn Implementation)}
    \label{fig:confusion_matrix_mlp_scikit_salary}
\end{figure}

We can see that the Logistic Regression model implemented with Scikit-Learn achieved
almost the same results as the manual implementation and the MLP model implemented
with Scikit-Learn. All 4 models achieved a high number of true positive predictions
for the negative class, indicating that the models are effective at identifying
negative instances in the dataset. The true positive predictions for the positive
class are lower, but the models still achieved a reasonable prediction but the
MLP model with manual implementation achieved the worst results. It may be due to
the hyperparameters that were set for the model. The false positive and false negative
predictions are also lower than the true positive and true negative predictions, which
is a good sign for the model's performance.

In Figures \ref{fig:confusion_matrix_logistic_manual_stroke} and
\ref{fig:confusion_matrix_logistic_scikit_stroke} we can see the confusion matrices
for the Logistic Regression model on the Stroke Prediction dataset, implemented
manually and with Scikit-Learn, respectively. In Figures
\ref{fig:confusion_matrix_mlp_manual_stroke} and \ref{fig:confusion_matrix_mlp_scikit_stroke}
we can see the confusion matrices for the Multi-Layered Perceptron model on the
Stroke Prediction dataset, implemented manually and with Scikit-Learn, respectively.

Because of the high class imbalance in the Stroke Prediction dataset, the models
can not predict the positive class effectively (the positive class is less than 5\%
of the dataset). On the other hand, the models can predict the negative class
effectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_AVC/LogisticRegression/confusion_matrix_manual.png}
    \caption{Confusion Matrix for the Logistic Regression model on the Stroke Prediction dataset (Manual Implementation)}
    \label{fig:confusion_matrix_logistic_manual_stroke}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_AVC/LogisticRegression/confusion_matrix_scikit-learn.png}
    \caption{Confusion Matrix for the Logistic Regression model on the Stroke Prediction dataset (Scikit-Learn Implementation)}
    \label{fig:confusion_matrix_logistic_scikit_stroke}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_AVC/MLP/confusion_matrix_manual.png}
    \caption{Confusion Matrix for the Multi-Layered Perceptron model on the Stroke Prediction dataset (Manual Implementation)}
    \label{fig:confusion_matrix_mlp_manual_stroke}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../tema2_AVC/MLP/confusion_matrix_scikit-learn.png}
    \caption{Confusion Matrix for the Multi-Layered Perceptron model on the Stroke Prediction dataset (Scikit-Learn Implementation)}
    \label{fig:confusion_matrix_mlp_scikit_stroke}
\end{figure}

\subsection{Evaluation Metrics}
To evaluate the performance of the machine learning models, we employ a range of
evaluation metrics that provide insights into different aspects of the model's
performance. These metrics include:

\begin{itemize}
    \item \textit{Accuracy:} The proportion of correctly classified instances
    out of the total instances. It provides a general measure of the model's
    correctness.
    \item \textit{Precision:} The proportion of true positive predictions out
    of all positive predictions. It measures the model's ability to avoid false
    positives.
    \item \textit{Recall:} The proportion of true positive predictions out of
    all actual positive instances. It measures the model's ability to capture
    all positive instances.
    \item \textit{F1-Score:} The harmonic mean of precision and recall. It provides
    a balanced measure of the model's performance.
\end{itemize}

These evaluation metrics help us understand the strengths and weaknesses of the
machine learning models and guide us in improving their performance. By analyzing
these metrics, we can identify areas for optimization and fine-tuning to enhance
the model's predictive capabilities.

In the following tables, Table \ref{tab:evaluation_metrics_salary} and Table
\ref{tab:evaluation_metrics_stroke}, we present the evaluation metrics for the 
Logistic Regression
and Multi-Layered Perceptron models on both prediction tasks: Salary Prediction and
Stroke Prediction.

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{4cm}||>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|}
        \hline
        \multicolumn{5}{|c|}{Evaluation Metrics for Salary Prediction Dataset - Train set} \\
        \hline
        Model & Accuracy & Precision & Recall & F1-Score \\
        \hline\hline
        Logistic Regression (Manual) & 0.829353669 & 0.700867052 & 0.504945340 & 0.586989409 \\
        \hline
        Logistic Regression (Scikit-Learn) & 0.8371046 & 0.707940780 & 0.54763144 & 0.617552098 \\
        \hline
        Multi-Layered Perceptron (Manual) & 0.759844980 & \textbf{1.0} & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Scikit-Learn) & \textbf{0.91098887} & 0.85537918 & \textbf{0.75741801} & \textbf{0.80342352} \\
        \hline
        \hline
        \multicolumn{5}{|c|}{Evaluation Metrics for Salary Prediction Dataset - Test set} \\
        \hline
        Model & Accuracy & Precision & Recall & F1-Score \\
        \hline\hline
        Logistic Regression (Manual) & \textbf{0.831} & 0.712250712 & 0.513347022 & 0.596658711 \\
        \hline
        Logistic Regression (Scikit-Learn) & 0.825 & 0.676092544 & \textbf{0.54004106} & \textbf{0.60045662} \\
        \hline
        Multi-Layered Perceptron (Manual) & 0.7565 & \textbf{1.0} & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Scikit-Learn) & 0.811 & 0.635235732 & 0.525667351 & 0.575280898 \\
        \hline
        \caption{Evaluation Metrics for Logistic Regression and Multi-Layered Perceptron Models on Salary Prediction Dataset}
        \label{tab:evaluation_metrics_salary} \\
    \end{longtable}
\end{center}

\begin{center}
    \begin{longtable}{ |>{\centering\arraybackslash}m{4cm}||>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|}
        \hline
        \multicolumn{5}{|c|}{Evaluation Metrics for Stroke Prediction Dataset - Train set} \\
        \hline
        Model & Accuracy & Precision & Recall & F1-Score \\
        \hline\hline
        Logistic Regression (Manual) & 0.957191780 & 0. & 0.0 & 0.0 \\
        \hline
        Logistic Regression (Scikit-Learn) & 0.957436399 & \textbf{1.0} & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Manual) & 0.957436399 & \textbf{1.0} & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Scikit-Learn) & \textbf{0.96232876} & 0.83333333 & \textbf{0.14367816} &  \textbf{0.24509803} \\
        \hline
        \hline
        \multicolumn{5}{|c|}{Evaluation Metrics for Stroke Prediction Dataset - Test set} \\
        \hline
        Model & Accuracy & Precision & Recall & F1-Score \\
        \hline\hline
        Logistic Regression (Manual) & 0.925636007 & 0.0 & 0.0 & 0.0 \\
        \hline
        Logistic Regression (Scikit-Learn) & 0.925636007 & 0.0 & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Manual) & \textbf{0.92661448} & \textbf{1.0} & 0.0 & 0.0 \\
        \hline
        Multi-Layered Perceptron (Scikit-Learn) & 0.92367906 & 0.2 & \textbf{0.01333333} & \textbf{0.025} \\
        \hline

        \caption{Evaluation Metrics for Logistic Regression and Multi-Layered Perceptron Models on Stroke Prediction Dataset}
        \label{tab:evaluation_metrics_stroke} \\
    \end{longtable}
\end{center}

% ------------------------------------------------------------------------------
\section{Conclusions}
In this document, we explored the application of machine learning algorithms to
two distinct prediction tasks: Salary Prediction and Stroke Prediction. We conducted
a comprehensive analysis of the datasets, including data exploration, feature
correlation analysis, and data preprocessing. We implemented two prominent machine
learning algorithms, Logistic Regression and Multi-Layered Perceptron (MLP), to
predict the outcomes of the two tasks. We evaluated the models using various
evaluation metrics, including accuracy, precision, recall, and F1-Score, to assess
their performance.

The results of the evaluation metrics indicate that the models achieved varying
levels of performance on the two prediction tasks. The Logistic Regression model
performed well on the Salary Prediction dataset, achieving high accuracy and F1-Score
values. The Multi-Layered Perceptron model also demonstrated strong performance on
the Salary Prediction dataset, achieving high accuracy and F1-Score values. However,
the models struggled to predict the positive class effectively
on the Stroke Prediction dataset due to the high class imbalance. The Logistic
Regression and Multi-Layered Perceptron models achieved similar results on the
Stroke Prediction dataset, with low recall and F1-Score values for the positive class.

Overall, it is evident that the Multi-Layered Perceptron (Scikit-Learn 
implementation) generally performs better than Logistic Regression across both 
datasets. Specifically:

\begin{itemize}
    \item \textit{Salary Prediction:} The MLP model achieves higher accuracy, 
    precision, recall, and F1-score on the training set compared to Logistic
     Regression. While its test set performance is slightly lower than its 
     training set performance, it still demonstrates a competitive F1-score.
    \item \textit{Stroke Prediction:} Despite the challenge posed by class 
    imbalance, the MLP model achieves a better balance between precision and 
    recall than Logistic Regression, which fails to predict the positive class 
    at all, resulting in an F1-score of 0.
\end{itemize}

Therefore, based on these metrics, the Multi-Layered Perceptron (Scikit-Learn 
implementation) is the better algorithm for both the Salary Prediction and 
Stroke Prediction tasks due to its superior performance across multiple 
evaluation criteria.


\pagebreak
% ------------------------------------------------------------------------------

% ---- Bibliography ----
\begin{thebibliography}{8}
    \bibitem{}
    \href{https://curs.upb.ro/2023/course/view.php?id=13749}{Moodle - Artifical Intelligence Course}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Machine_learning}{Wikipedia - Machine Learning}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Logistic_regression}{Wikipedia - Logistic Regression}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Multilayer_perceptron}{Wikipedia - Multilayer Perceptron}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Ridge_regression}{Wikipedia - Ridge Regression}
    \bibitem{}
    \href{https://www.semanticscholar.org/reader/2c9022fe0af15568a885e59d475ec8f95726e51b}{Metrics for Multi-Class Classification: An Overview}
    \bibitem{}
    \href{https://developers.google.com/machine-learning/crash-course/classification/accuracy}{Google Developers - Classification: Accuracy}
    \bibitem{}
    \href{https://ocw.mit.edu/courses/6-867-machine-learning-fall-2006/}{MIT OpenCourseWare - Machine Learning}
    \bibitem{}
    \href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html}{Pandas - DataFrame}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Imputation_(statistics)}{Wikipedia - Imputation (statistics)}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Correlation}{Wikipedia - Correlation}
    \bibitem{}
    \href{https://thinkingneuron.com/how-to-measure-the-correlation-between-two-categorical-variables-in-python/}{Thinking Neuron - How to Measure the Correlation Between Two Categorical Variables in Python}
    \bibitem{}
    \href{https://www.stratascratch.com/blog/chi-square-test-in-python-a-technical-guide/}{StrataScratch - Chi-Square Test in Python: A Technical Guide}
    \bibitem{}
    \href{https://en.wikipedia.org/wiki/Cram\%C3\%A9r\%27s\_V}{Wikipedia - Cramér's V}
    \bibitem{}
    \href{https://saturncloud.io/blog/how-to-detect-and-exclude-outliers-in-a-pandas-dataframe/}{Saturn Cloud - How to Detect and Exclude Outliers in a Pandas DataFrame}
    \bibitem{}
    \href{https://www.kaggle.com/discussions/questions-and-answers/159183}{Kaggle - When to standardize test and train data?}
    \bibitem{}
    \href{https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling}{Scikit-Learn - Standardization or Mean Removal and Variance Scaling}
    \bibitem{}
    \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html}{Scikit-Learn - OneHotEncoder}
    \bibitem{}
    \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html}{Scikit-Learn - LabelEncoder}
    \bibitem{}
    \href{https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62}{Towards Data Science - Understanding Confusion Matrix}
    
    \end{thebibliography}
\end{document}